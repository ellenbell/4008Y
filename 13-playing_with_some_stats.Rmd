# Playing with some statistics - Workshop 5 - Athletes continued, Week 4B

Here we have material for Workshop 5 in Week 4 of Semester 2. We will be continuing with the Athletes data set from Week 2. So go to your `athletes` project in posit Cloud. It may be helpful to review the plots you made (including the histograms) to remind yourself of how the data look. 

## Descriptive statistics

At the end of the Autumn Semester we started to discus some simple descriptive statistics, including measures of central tendency and spread. We discussed means, medians, modes, ranges, standard deviations, standard error of the mean and 95% confidence intervals. If you cant remember what any of these are it is strongly recommended that you revisit the lecture materials. In this section we will be learning how to calculate these basic descriptive statistics for the athletes data set.  

### Task 1 - Calculating some descriptive statistics 

Copy the following piece of code into your script and then run it;

```{r}
# Create a new object called athletes_summary_stats_ht and store mean, standard deviation (sd), and sample size (n) for height (ht) in male and female athletes.
athletes_summary_stats_ht <- athletes %>% 
  group_by(sex) %>% 
  summarise(mean=mean(ht),
            sd=sd(ht),
            n=n())
athletes_summary_stats_ht
```

Have a look at the table that R produces. Then look back at the code you used, do you understand what each function has done? The `summarise` function can also calculate other summary statistics, use `help(summarise)` to find out what else it can do. Play with the function, see if you can produce a similar table that calculates the mean and standard deviation for red blood cell count in male and female athletes. 

It would also be useful to know the standard error of the mean and 95% confidence intervals for this data set. We can use the `mutate` function to add additional columns onto our summary stats table. Remember we can calculate the standard error of the mean using;

$$
SEM = \frac{SD}{\sqrt{n}}
$$
Copy the following chunk of code into your script, run it, and then take a look at the `athletes_summary_stats_ht` object. 

```{r}
athletes_summary_stats_ht <- athletes_summary_stats_ht %>%
  mutate(sem = sd/sqrt(n))
```

Hopefully you should see something that looks like this:

```
> athletes_summary_stats_ht
# A tibble: 2 × 7
  sex    mean    sd     n   sem 
  <chr> <dbl> <dbl> <int> <dbl>
1 f      175.  8.24   100 0.824
2 m      186.  7.90   102 0.783 
```

Now try using the `mutate` function to calculate the 95% confidence intervals. Remember 95% confidence intervals fall on either side of the mean, so you will need an upper and lower bound, so you will need to use mutate twice, once for an `upper_ci` and once for a `lower_ci`. 

The equation for calculating 95% confidence intervals is:

$$
95percent CI = \ Mean ± (1.96 * SEM)
$$

Take a look at your `athletes_summary_stats_ht` object. It should hopefully look something like this:

```
> athletes_summary_stats_ht
# A tibble: 2 × 7
  sex    mean    sd     n   sem upper_ci lower_ci
  <chr> <dbl> <dbl> <int> <dbl>    <dbl>    <dbl>
1 f      175.  8.24   100 0.824     176.     173.
2 m      186.  7.90   102 0.783     187.     184.
```

The skills you have learned here could be applied to any of the numeric variables in the `athletes` data set. Try calculating similar values for the red blood cell count variable. 

## Introducing some inferential statistics

We have, so far, looked at how to explore our data set, both graphically (using histograms, box plots and scatter plots) and numerically (by using descriptive statistics). But something you will come into frequent contact with throughout your degree is inferential statistics. You may heave heard of statistical tests such as; T-test, ANOVA, Chi squared and Mann Whitney U before. These are all different types of statistical model that are loosely grouped under the header of inferential statistics. These are statistics that allow you to make predictions from your data with the aim of allowing you to take data from your samples and make generalisations about a respective population. You may hear people refer to *significant differences* or *significant relationships*, essentially as soon as the term *significant* comes into play there is an assumption made that some kind of statistical test has been applied. 

We wont be going into a huge amount of detail regarding these inferential statistics, the field of statistics is huge and there are many ways you may apply statistics to different data sets. At this stage, I would much prefer that you become confident with visualising data and interpreting those visualisations. However to ensure that you are set up for your future studies and data analysis ambitions it important that you are at least aware of some inferential statistics. My aim here is go give you an overview of how to run and interpret the statistical models that you are most likely to come across in your degree. This will hopefully act as a foundation in statistic that you can build on as required. 

### The general linear model 

General linear models (not to be confused with generalised linear models or GLMs) are a commonly used statistic in the biological sciences, this is, at least in part, because they are pretty versatile and can output information on statistical significance and effect size. We wont go into the maths behind them in too much detail but in essence they make use of a linear equation. We can apply general linear models to look for differences, if one of our variables is categorical (essentially we are performing an ANOVA or T-test), this will compare the means between our two groups, or we can use the same function to look for relationships and perform a regression, if both of our variables are continuous. 

Helpfully, general linear models are very easy to apply in R, there is a function called `lm()` that is part of the base R package. Lets have a look at some applications of this function. 

### Testing for differences 

Lets say we wanted to analyse the difference in average weight between male and female athletes. So our hypothesis might be; male athletes are heavier then female athletes, in this case the predictor variable is sex and the response variable is weight. If we wanted to use that information to fit a general linear model we could use the following chunk;   

```{r}
# Fitting a linear model 
# Here I have created an object "linear_model01" to store the outputs of our model in
linear_model01 <- lm(wt ~ sex, data = athletes) # lm() is the function here, and we are specifying that we want to analyse weight (our response variable) as a function of sex (our predictor variable) using tilde (~). It is super important to place your response and predictor variables on the right side of tilde. We then just tell R which data frame we are using with the data = athletes argument. 
summary(linear_model01) # Summary() will just print out a summary of our model for us to interpret 
```

The `summary()` function will then provide you with the following overview of your model output; 

```
> summary(lsmodel01)

Call:
lm(formula = wt ~ sex, data = athletes)

Residuals:
    Min      1Q  Median      3Q     Max 
-29.542  -7.703   0.517   7.538  40.676 

Coefficients:
            Estimate Std. Error t value Pr(>|t|)    
(Intercept)   67.342      1.169  57.599   <2e-16 ***
sexm          15.182      1.645   9.227   <2e-16 ***
---
Signif. codes:  0 ‘***’ 0.001 ‘**’ 0.01 ‘*’ 0.05 ‘.’ 0.1 ‘ ’ 1

Residual standard error: 11.69 on 200 degrees of freedom
Multiple R-squared:  0.2986,	Adjusted R-squared:  0.2951 
F-statistic: 85.14 on 1 and 200 DF,  p-value: < 2.2e-16
```

There is a lot of information here so we will break each section down. 

First of all we have a reminder of the formula we gave to the `lm()` function;
```
Call:
lm(formula = wt ~ sex, data = athletes)
```

Then we have a summary of our residuals;
```
Residuals:
    Min      1Q  Median      3Q     Max 
-29.542  -7.703   0.517   7.538  40.676 
```

I wont go into a huge amount of detail around how we interpret these, you can read around it if it interests you. But in a nutshell the general linear model draws a straight line through all of our data points, so it has fitted a model predicting where it would expect your data points to land. The residuals are then the distances away from that line each data point is, so the distance each of your observed data points are away from the predicted values. Here R has given us some summary statistics around the residuals. 

Next we have our coefficients;

```
Coefficients:
            Estimate Std. Error t value Pr(>|t|)    
(Intercept)   67.342      1.169  57.599   <2e-16 ***
sexm          15.182      1.645   9.227   <2e-16 ***
---
Signif. codes:  0 ‘***’ 0.001 ‘**’ 0.01 ‘*’ 0.05 ‘.’ 0.1 ‘ ’ 1

```

So here we have a table with two rows called `(Intercept)` and `sexm` and four collumns called `Estimate` which is essentially the mean, `Std. Error` which is standard error, `t value` which is our T statistic and `Pr(>ltl)` which is otherwise known as a p value. We set out to model the difference in weight between male and female athletes, the labeling here is somewhat confusing but the `(Intercept)` row actually refers to the female athletes. You can check for yourself if you like the `Estimate` and `Std. Error` here denote the mean and standard error for weight among female athletes. The`t value` is the `Estimate` divided by `Std. Errors` and the `Pr(>|t|)` is the corresponding value for that t value in a T distribution table with the given degrees of freedom (dont worry about this last bit just yet).

A common mistake is therefore to assume that the `sexm` row then referes to the same values for male athletes. This isn't quite right. This actually refers to the **difference in the mean weight of the two groups.** This focuses then on the question which we are asking: is their a difference in the weight between male and female athletes? You can check this for yourself, but essentially this row is saying that male athletes are, on average, 15.2kgs heavier then female athletes with a standard error of 1.65kgs. Our `t value` here is then the `Estimate` divided by the `Std. Error` again. 

Now lets have a look at our `Pr(>ltl)` value, otherwise known as a p value. I am not going to go into how these are calculated as they are often worked out by comparing the test statistics (in this case the T value) and degrees of freedom, against a contingency table. However I will give you some outline of how they are interpreted. Our p value here is given as >2e-16 which translates as 0.0000000000000002. We can interpret this value as there is a 0.0000000000000002% probability that we would see our given t value if the null hypothesis was true, out null hypothesis being that there is no difference in weight between male and female athletes. This, I hope you will agree is a very low probability. We generally use cut offs for p values of 0.05 or 0.01 so if you get a p value less then those cut offs you have found statistical significance. Here our p value is less than 0.01 so we can say that there is a statistically significant difference between male and female athlete weight.

Finally you may have noticed that there are some asterisks `*` in the coefficients section, these relate to the `Signif.codes`, or significance codes row below. Here you can see the relavent number of asterisks and how they relate to the different p value cutoffs. 

The last part of our summary is shown here;

```
Residual standard error: 11.69 on 200 degrees of freedom
Multiple R-squared:  0.2986,	Adjusted R-squared:  0.2951 
F-statistic: 85.14 on 1 and 200 DF,  p-value: < 2.2e-16
```

The residual standard error is pretty much what is says on the tin. I'm not going to go into more depth on residuals here. But I will draw your attention to degrees of freedom or DF. This is essentially is the number of independent pieces of information used to calculate a statistic. It’s calculated as the sample size minus the number of restrictions. So in a nutshell its a measure of sample size. 

The multiple R-squared value describes how well your regression model explains the variation in your data. Here we have a multiple R-squared of 0.2986, which we could round to 0.3. This can be interpreted as 30% of the variation in weight is explained by the sex of the athletes. Adjusted R-squared, which is also reported here is only really of use if you are doing multivariate statistics it takes into account how many samples you have and how many variables you’re using. Here we only have one variable so we don't need to worry about the adjusted R-squared. 

Finally we have our F statistic. This is essentially another test statistic (like T in the coefficients table), here it has been reported with its own degrees of freedom (DF) and p value. You can interpret these as we did previously. 

So by funning the `lm()` function on categorical data and using the `summary()` function, we have performed both an Independent T-test (T-values in the coefficient section) and an ANOVA (the F statistics). We have also aquired an R-squared value to show the goodness of fit of our regression. 

Try running the `lm()` and `summary()` functions now height and sex in the athletes data set, can you interpret the results?

### Testing for relationships


### Assumptions

We have explored several applications of the general linear model (including T-tests, ANOVA and regression). However you should also be aware of some of the assumptions that these models make. All statistical models come with a set of assumptions that are made of the data set. As a result your choice of test is quite important when starting to perform inferential statistics, and there are a number of factors around your data that you need to be clear about. These include;

* What is your question and hypothesis?
* What types of data are available to you in your data set?
* What is your sample size?
* Are your data normally distributed?

Most of these fairly self explanatory and easy to find out but a key assumption made by all the statistical tests we have looked at so far and that is a little more tricky to define is that the data are normally distributed. 

You will frequently see statistical models refereed to as parametric or non-parametric. 

|   Parametric      |  Non-Parametric  |   
|:-----------------:|:----------------:|
|General linear model variants |      |
|Independent T-test |  Mann-Whitney U  |     
|     ANOVA         |  Mann-Whitney U  | 
|    Pearson's Correlation        |     Spearman's Rank Correlation    | 

Parametric tests assume that your data are normally distributed and non-parametric tests do not make this assumption, but are not as statistically powerful as parametric tests (hense why we don't always do a non-parametric test just to be on the safe side).

To check our distributions we can and should always make a histogram just to look over the data. But there are also tests we can use to reassure ourselves of the data distribution. One such test is called the Shapiro-Wilk test for normality and its very easy to perform in R. Lets go back to our weight variable in the male athletes data set and see if that is normally distributed. First of all pull up your histogram for this variable and remind yourself of how the data are distributed. 

Now to add confidence to our interpretation we can run the following piece of code;

```{r}
# Normality test

shapiro.test(male_athletes$wt) # Notice we are running this test on just the male athletes

```

The results should look something like this;

```
> shapiro.test(athletes$wt)

	Shapiro-Wilk normality test

data:  athletes$wt
W = 0.99299, p-value = 0.4516

```

Here our test statistic is W and our p value is fairly self explanatory. But the interpretation of this test is often a little challenging for students. Here the null hypothesis is that our data are normally distributed so if our p value is greater then 0.05 we accept the null hypothesis to be true and our data are normally distributed but if our p value is less than 0.05 our null hypothesis is rejected and that our data are not normally distributed. Here because weight is normally distributed it would be fine to run a parametric test on it. 

Try running this test on all of your variables for male and female athletes, are any of them not normally distributed?

### What to do if your data are not normally distributed

There are a couple of things we can do if our data are not normally distributed. 

1) We can log transform the data
2) We can run a non parametric test

Hopefully you noticed that the red blood cell count for both male and female athletes was not normally distributed. We can log transform the female althletes red blood cell count using the following piece of code;

```{r}
# Log transform

female_athletes <- female_athletes %>%
  mutate(log10 = log10(female_athletes$rcc))
```

Now run the Shapiro-Wilk test on this new log10 variable and see if its normally distributed.

### Testing for Correlations

```{r}
# Normality and homology of varience test

shapiro.test(athletes$ht)
shapiro.test(athletes$wt)
shapiro.test(male_athletes$rcc)
shapiro.test(female_athletes$rcc)



# linear models 

lsmodel01 <- lm(ht ~ sex, data = athletes)
summary(lsmodel01)
lsmodel02 <- lm(wt ~ sex, data = athletes)
summary(lsmodel02)
lsmodel03 <- lm(wt ~ ht, data = athletes)
summary(lsmodel03)

# Correlations

correlation01 <- cor.test(athletes$ht, athletes$wt, method = "pearson")
correlation01

# Log transform

female_athletes <- female_athletes %>%
  mutate(log10 = log10(female_athletes$rcc))
male_athletes <- male_athletes %>% 
  mutate(log10 = log10(male_athletes$rcc))
shapiro.test(female_athletes$log10)
shapiro.test(male_athletes$log10)

# Mann Whitney U / Wilcoxan 

wilcox.test(rcc ~ sex, data=athletes) 

```

```
	Shapiro-Wilk normality test

data:  athletes$log10
W = 0.97946, p-value = 0.004685

> leveneTest(athletes$ht, athletes$sex)
Levene's Test for Homogeneity of Variance (center = median)
       Df F value Pr(>F)
group   1  0.0955 0.7577
      200 

> wilcox.test(rcc ~ sex, data=athletes) 

	Wilcoxon rank sum test with continuity correction

data:  rcc by sex
W = 948, p-value < 2.2e-16
alternative hypothesis: true location shift is not equal to 0
```
